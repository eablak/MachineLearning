{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f857831e",
   "metadata": {},
   "source": [
    "#### SENTENCE SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14379111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "mytext = \"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. It is the series of steps involved in building any NLP model. These steps are common in every NLP project, so it makes sense to study them in this chapter. Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace. Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process. In this chapter, we will learn about the various steps involved and how they playimportant roles in solving the NLP problem and we’ll see a few guidelines about when and how to use which step. In later chapters, we’ll discuss specific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\n",
    "my_sentences = sent_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b6ab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.',\n",
       " 'If we were asked to build such an application, think about how we would approach doing so at our organization.',\n",
       " 'We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.',\n",
       " 'Since language processing is involved, we would also list all the forms of text processing needed at each step.',\n",
       " 'This step-by-step processing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in building any NLP model.',\n",
       " 'These steps are common in every NLP project, so it makes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.',\n",
       " 'In this chapter, we will learn about the various steps involved and how they playimportant roles in solving the NLP problem and we’ll see a few guidelines about when and how to use which step.',\n",
       " 'In later chapters, we’ll discuss specific pipelines for various NLP tasks (e.g., Chapters 4–7).']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f510cb",
   "metadata": {},
   "source": [
    "#### WORD TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4c7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to build such an application, think about how we would approach doing so at our organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this chapter, we will learn about the various steps involved and how they playimportant roles in solving the NLP problem and we’ll see a few guidelines about when and how to use which step.\n",
      "['In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'playimportant', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '’', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we’ll discuss specific pipelines for various NLP tasks (e.g., Chapters 4–7).\n",
      "['In', 'later', 'chapters', ',', 'we', '’', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4–7', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fcb0d",
   "metadata": {},
   "source": [
    "#### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3539fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e7c2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK\n",
      "Tokenized corpus: ['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', '&', 'should', 'be', 'done', 'soon', '!', '!', '.', 'It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', '.', 'But', 'will', 'it', '?', 'This', 'notebook', 'has', 'been', 'run', '4', 'times', '!', '!']\n",
      "\n",
      "Tokenized corpus without stopwords:  ['Need', 'finalize', 'demo', 'corpus', 'used', 'notebook', '&', 'done', 'soon', '!', '!', '.', 'It', 'done', 'ending', 'month', '.', 'But', '?', 'This', 'notebook', 'run', '4', 'times', '!', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ESRA\n",
      "[nltk_data]     ABLAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ESRA\n",
      "[nltk_data]     ABLAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words_nltk = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"\\nTokenized corpus without stopwords: \", tokenized_corpus_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab700d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea04666",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015b8453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy:\n",
      "\n",
      "Tokenized Corpus: ['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', '&', 'should', 'be', 'done', 'soon', '!', '!', '.', 'It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', '.', 'But', 'will', 'it', '?', 'This', 'notebook', 'has', 'been', 'run', '4', 'times', '!', '!']\n",
      "\n",
      "Tokenized corpus without stopwords ['Need', 'finalize', 'demo', 'corpus', 'notebook', '&', 'soon', '!', '!', '.', 'It', 'ending', 'month', '.', 'But', '?', 'This', 'notebook', 'run', '4', 'times', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print(\"\\nTokenized Corpus:\",tokenized_corpus_spacy)\n",
    "tokens_without_sw = [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "print(\"\\nTokenized corpus without stopwords\", tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "283b1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between NLTK and spaCy output:\n",
      " {'used', 'done'}\n"
     ]
    }
   ],
   "source": [
    "print(\"difference between NLTK and spaCy output:\\n\",\n",
    "     set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0638ff",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de035dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook & should be done soon ! ! . it should be done by the end of thi month . but will it ? thi notebook ha been run 4 time ! ! "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(corpus)\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe1b0d",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3a1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\ESRA\n",
      "[nltk_data]     ABLAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec7f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to finalize the demo corpus which will be used for this notebook & should be done soon ! ! . It should be done by the ending of this month . But will it ? This notebook ha been run 4 time ! ! "
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acba02b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better well\n"
     ]
    }
   ],
   "source": [
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "token = sp(u\"better\")\n",
    "for word in token:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e329eb9",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca01110",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca20fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using spacy:\n",
      "Need : VERB\n",
      "to : PART\n",
      "finalize : VERB\n",
      "the : DET\n",
      "demo : NOUN\n",
      "corpus : NOUN\n",
      "which : PRON\n",
      "will : AUX\n",
      "be : AUX\n",
      "used : VERB\n",
      "for : ADP\n",
      "this : DET\n",
      "notebook : NOUN\n",
      "and : CCONJ\n",
      "it : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "soon : ADV\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      ". : PUNCT\n",
      "It : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "by : ADP\n",
      "the : DET\n",
      "ending : NOUN\n",
      "of : ADP\n",
      "this : DET\n",
      "month : NOUN\n",
      ". : PUNCT\n",
      "But : CCONJ\n",
      "will : AUX\n",
      "it : PRON\n",
      "? : PUNCT\n",
      "This : DET\n",
      "notebook : NOUN\n",
      "has : AUX\n",
      "been : AUX\n",
      "run : VERB\n",
      "4 : NUM\n",
      "times : NOUN\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      "POS Tagging using NLTK\n",
      "[('Need', 'NN'), ('to', 'TO'), ('finalize', 'VB'), ('the', 'DT'), ('demo', 'NN'), ('corpus', 'NN'), ('which', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('this', 'DT'), ('notebook', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('soon', 'RB'), ('!', '.'), ('!', '.'), ('.', '.'), ('It', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('ending', 'VBG'), ('of', 'IN'), ('this', 'DT'), ('month', 'NN'), ('.', '.'), ('But', 'CC'), ('will', 'MD'), ('it', 'PRP'), ('?', '.'), ('This', 'DT'), ('notebook', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('run', 'VBN'), ('4', 'CD'), ('times', 'NNS'), ('!', '.'), ('!', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading avaraged_perceptron_tagger: Package\n",
      "[nltk_data]     'avaraged_perceptron_tagger' not found in index\n"
     ]
    }
   ],
   "source": [
    "print(\"POS Tagging using spacy:\")\n",
    "doc = spacy_model(corpus_original)\n",
    "# Token and Tag\n",
    "for token in doc:\n",
    "    print(token,\":\",token.pos_)\n",
    "    \n",
    "# pos tagginf using nltk\n",
    "nltk.download(\"avaraged_perceptron_tagger\")\n",
    "print(\"POS Tagging using NLTK\")\n",
    "print(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
